install.packages(c("boot", "class", "foreign", "KernSmooth", "MASS", "nlme", "nnet", "spatial"))
install.packages("pagedown")
help(html_resume)
getwd()
install.packages('pagedown')
install.packages("pagedown")
getwd()
setwd("~/")
setwd("~/")
install.packages(c("AsioHeaders", "backports", "bookdown", "fs", "ggplot2", "htmltools", "isoband", "jsonlite", "knitr", "openssl", "pillar", "pkgbuild", "processx", "promises", "Rcpp", "rlang", "rmarkdown", "survival", "tibble", "tidytext", "tinytex", "vctrs", "websocket", "xfun"))
install.packages("cheatdown")
install.packages("ranger")
install.packages(c("AsioHeaders", "backports", "BH", "bookdown", "boot", "callr", "class", "cli", "clipr", "cluster", "codetools", "colorspace", "crayon", "desc", "digest", "dplyr", "fansi", "farver", "foreign", "fs", "generics", "ggplot2", "gh", "git2r", "glue", "gutenbergr", "hms", "htmltools", "httpuv", "httr", "hunspell", "isoband", "ISOcodes", "jsonlite", "KernSmooth", "knitr", "labeling", "lifecycle", "magrittr", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "openssl", "pagedown", "pillar", "pkgbuild", "pkgload", "processx", "promises", "ps", "R6", "Rcpp", "RcppEigen", "readr", "rlang", "rmarkdown", "rprojroot", "rstudioapi", "servr", "spatial", "stopwords", "stringi", "survival", "sys", "testthat", "tibble", "tidyr", "tidytext", "tinytex", "usethis", "vctrs", "websocket", "withr", "xfun"))
transpose
trasnpose a matrix
install.packages(c("bookdown", "boot", "brio", "callr", "class", "cli", "cluster", "colorspace", "cpp11", "credentials", "curl", "desc", "diffobj", "digest", "dplyr", "ellipsis", "fansi", "gert", "ggplot2", "gh", "gutenbergr", "highr", "hms", "htmltools", "httpuv", "isoband", "KernSmooth", "knitr", "later", "lattice", "lifecycle", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "openssl", "pagedown", "pillar", "pkgload", "processx", "R6", "ranger", "Rcpp", "readr", "rlang", "rmarkdown", "servr", "spatial", "stringi", "survival", "testthat", "tibble", "tidyr", "tidyselect", "tidytext", "tinytex", "usethis", "utf8", "vctrs", "viridisLite", "waldo", "websocket", "withr", "xfun", "zip"))
install.packages(c("fable", "tsibble"))
install.packages("tseries")
install.packages("fpp3")
install.packages("TSA")
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fpp3)
library(tseries)
library(tsibble)
library(TSA)
leisure <- us_employment %>%
filter(Title == "Leisure and Hospitality",
year(Month) > 2000) %>%
mutate(Employed = Employed/1000) %>%
select(Month, Employed)
autoplot(leisure, Employed) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
leisure %>%
gg_tsdisplay(difference(Employed, 12),
plot_type='partial', lag=36) +
labs(title="Seasonally differenced", y="")
leisure %>%
gg_tsdisplay(difference(Employed, 12) %>% difference(),
plot_type='partial', lag=36) +
labs(title = "Double differenced", y="")
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fpp3)
library(tseries)
library(tsibble)
library(TSA)
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
glance(fit) %>% arrange(AICc) %>% select(.model:BIC)
fit %>% select(auto) %>% gg_tsresiduals(lag=36)
augment(fit) %>% features(.innov, ljung_box, lag=24, dof=4)
forecast(fit, h=36) %>%
filter(.model=='auto') %>%
autoplot(leisure) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fpp3)
library(tseries)
library(tsibble)
library(TSA)
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fpp3)
library(tseries)
library(tsibble)
library(TSA)
leisure <- us_employment %>%
filter(Title == "Leisure and Hospitality",
year(Month) > 2000) %>%
mutate(Employed = Employed/1000) %>%
select(Month, Employed)
autoplot(leisure, Employed) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
leisure %>%
gg_tsdisplay(difference(Employed, 12),
plot_type='partial', lag=36) +
labs(title="Seasonally differenced", y="")
leisure %>%
gg_tsdisplay(difference(Employed, 12) %>% difference(),
plot_type='partial', lag=36) +
labs(title = "Double differenced", y="")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
fit
mable?
``
help(mable)
install.packages("fabletools")
install.packages("fabletools")
install.packages("fabletools")
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fabletools)
library(fpp3)
library(tseries)
library(tsibble)
library(TSA)
leisure <- us_employment %>%
filter(Title == "Leisure and Hospitality",
year(Month) > 2000) %>%
mutate(Employed = Employed/1000) %>%
select(Month, Employed)
autoplot(leisure, Employed) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
leisure %>%
gg_tsdisplay(difference(Employed, 12),
plot_type='partial', lag=36) +
labs(title="Seasonally differenced", y="")
leisure %>%
gg_tsdisplay(difference(Employed, 12) %>% difference(),
plot_type='partial', lag=36) +
labs(title = "Double differenced", y="")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
helP(tibble)
help(tibble)
install.packages("tibble")
install.packages("tibble")
knitr::opts_chunk$set(echo = TRUE)
library(fable)
library(fabletools)
library(fpp3)
library(tibble)
library(tseries)
library(tsibble)
library(TSA)
leisure <- us_employment %>%
filter(Title == "Leisure and Hospitality",
year(Month) > 2000) %>%
mutate(Employed = Employed/1000) %>%
select(Month, Employed)
autoplot(leisure, Employed) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
leisure %>%
gg_tsdisplay(difference(Employed, 12),
plot_type='partial', lag=36) +
labs(title="Seasonally differenced", y="")
leisure %>%
gg_tsdisplay(difference(Employed, 12) %>% difference(),
plot_type='partial', lag=36) +
labs(title = "Double differenced", y="")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
glance(fit) %>% arrange(AICc) %>% select(.model:BIC)
fit %>% select(auto) %>% gg_tsresiduals(lag=36)
augment(fit) %>% features(.innov, ljung_box, lag=24, dof=4)
forecast(fit, h=36) %>%
filter(.model=='auto') %>%
autoplot(leisure) +
labs(title = "US employment: leisure and hospitality",
y="Number of people (millions)")
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
report(fit)
fit <- leisure %>%
model(
arima012011 = ARIMA(Employed ~ pdq(0,1,2) + PDQ(0,1,1)),
arima210011 = ARIMA(Employed ~ pdq(2,1,0) + PDQ(0,1,1)),
auto = ARIMA(Employed, stepwise = FALSE, approx = FALSE)
)
fit %>% pivot_longer(everything(), names_to = "Model name",
values_to = "Orders")
install.packages(c("backports", "cli", "crayon", "generics", "pillar", "rlang", "stopwords", "tsibble", "tzdb", "usethis", "xfun"))
install.packages("bookdown")
getwd()
getwd()
getwd()
setwd("~/R_exercises/Coursera/Course_5/week2/RepData_PeerAssessment1")
shiny::runApp('R_exercises/Coursera/Course_9/Anaheim_point_interest')
library(dplyr)
library(leaflet)
runApp('R_exercises/Coursera/Course_9/Anaheim_point_interest')
runApp('R_exercises/Coursera/Course_9/Anaheim_point_interest')
getwd()
setwd("~/R_exercises/Coursera/Course_10/LanguageModel")
require(readtext)
require(quanteda)
require(data.table)
require(stringi)
require(stringr)
path = "~/R_exercises/Coursera/Course_10/"
sample_size = 0.70
news <- stri_split_lines1(readtext(paste0(path,"final/en_US/en_US.news.txt")))
news <- sample(news, size=length(news)*sample_size)
blogs <- stri_split_lines1(readtext(paste0(path,"final/en_US/en_US.blogs.txt")))
blogs <- sample(blogs, size=length(blogs)*sample_size)
twitter <- stri_split_lines1(readtext(paste0(path,"final/en_US/en_US.twitter.txt")))
twitter <- sample(twitter, size=length(twitter)*sample_size)
txt <- c(news, blogs, twitter)
## Returns a Quanteda dfm from a given character vector
##
## txt - Character vector of text, each element in the vector is a document in dfm
## ng - The 'N' of N-gram
library(quanteda)
load.file.to.dfm <- function(txt, ng) {
text.dfm <- txt %>%
tokens(remove_numbers=T, remove_punct=T,
remove_symbols=T, remove_hyphens=T,
remove_twitter=T, remove_url=T) %>%
tokens_remove(stopwords("en")) %>%
tokens_ngrams(n=ng) %>% dfm()
return(text.dfm)
}
UniG <- load.file.to.dfm(txt, 1)  # dfm containing unigrams
BiG <- load.file.to.dfm(txt, 2)  # dfm containing bigrams
TriG <- load.file.to.dfm(txt, 3)  # dfm containing trigrams
CountNGramFreq <- function(NGrDfm) {
FreqV <- colSums(NGrDfm)
return(data.table(term=names(FreqV), c=FreqV))
}
UniFreq <- CountNGramFreq(UniG)
BiFreq <- CountNGramFreq(BiG)
TriFreq <- CountNGramFreq(TriG)
# To control which terms to be ignored with raw count < min_count
min_count = 10   #4
UniFreq <- UniFreq[c>min_count,]
BiFreq <- BiFreq[c>min_count,]
TriFreq <- TriFreq[c>min_count,]
## Calculate the "frequency of frequency r" (N_r)
CountNC <- function(FreqVec) {
CountTbl <- table(FreqVec[,.(c)])
return(data.table(cbind(c=as.integer(names(CountTbl)), Nr=as.integer(CountTbl))))
}
UniBins <- CountNC(UniFreq)
BiBins <- CountNC(BiFreq)
TriBins <- CountNC(TriFreq)
## Average non-zero count, replace N_r with Z_r
avg.zr <- function(Bins) {
max <- dim(Bins)[1]
r<-2:(max-1)
Bins[1, Zr:=2*Nr/Bins[2,c]]  # r=1, q=0, Zr=Nr/(0.5t)
Bins[r, Zr:=2*Nr/(Bins[r+1,c]-Bins[r-1,c])]  # else, Zr=Nr/(0.5(t-q))
Bins[max, Zr:=Nr/(c-Bins[(max-1),c])]  # r=max, t=2r-q, Zr=Nr/(r-q)
}
avg.zr(UniBins)
avg.zr(BiBins)
avg.zr(TriBins)
## Replace Z_r with value computed from a linear regression that is fit to map Z_r to c in log space
## log(Z_r) = a + b*log(c)
FitLM <- function(CountTbl) {
return(lm(log(Zr) ~ log(c), data = CountTbl))
}
UniLM <- FitLM(UniBins)
BiLM <- FitLM(BiBins)
TriLM <- FitLM(TriBins)
## Only perform the discounting to small count (c) n-grams, where c <= k, using Katz's formula
k=5
Cal_GTDiscount <- function(cnt, N) {
if (N==1) {
model <- UniLM
} else if (N==2) {
model <- BiLM
} else if (N==3) {
model <- TriLM
}
# Common parts
Z1 <- exp(predict(model, newdata=data.frame(c=1)))
Zr <- exp(predict(model, newdata=data.frame(c=cnt)))
Zrp1 <- exp(predict(model, newdata=data.frame(c=(cnt+1))))
Zkp1 <- exp(predict(model, newdata=data.frame(c=(k+1))))
sub <- ((k+1)*Zkp1)/(Z1)
new_r <- ((cnt+1)*(Zrp1)/(Zr)-cnt*sub)/(1-sub)
return(new_r)
}
UpdateCount <- function(FreqTbl, N) {
FreqTbl[c>k ,cDis:=as.numeric(c)]
FreqTbl[c<=k, cDis:=Cal_GTDiscount(c, N)]
}
UpdateCount(UniFreq, 1)
UpdateCount(BiFreq, 2)
UpdateCount(TriFreq, 3)
setkey(UniFreq, term)
setkey(BiFreq, term)
setkey(TriFreq, term)
## Return all the observed N-grams given the previous (N-1)-gram
##
## - wordseq: character vector of (N-1)-gram separated by underscore, e.g. "x1_x2_..._x(N-1)"
## - NgramFreq: datatable of N-grams
get.obs.NGrams.by.pre <- function(wordseq, NgramFreq) {
PreTxt <- sprintf("%s%s%s", "^", wordseq, "_")
NgramFreq[grep(PreTxt, NgramFreq[,term], perl=T, useBytes=T),]
}
## Return all the unigrams that end unobserved Ngrams
get.unobs.Ngram.tails <- function(ObsNgrams, N) {
ObsTails <- str_split_fixed(ObsNgrams[,term], "_", N)[,N]
return(data.table(term=UniFreq[!ObsTails,term,on="term"]))
}
## Compute the probabilities of observed N-gram.
## We need the counts from (N-1)-gram table since corpus doesn't include <EOS> explicitly,
## therefore the denominator will be smaller if only summing up all the terms
## from N-gram table
cal.obs.prob <- function(ObsNgrams, Nm1Grams, wordseq) {
PreCount <- Nm1Grams[wordseq, c, on=.(term)]
ObsNgrams[,Prob:=ObsNgrams[,cDis]/PreCount]  # c_dis/c
}
## Compute Alpha
## Return the normalization factor Alpha
##
## - ObsNgrams: datatable contains all observed ngrams starting with wordseq
## - Nm1Grams: datatable of (N-1)-grams containing count of wordseq
## - wordseq: an observed history: w_{i-N+1}^{i-1}
cal.alpha <- function(ObsNGrams, Nm1Grams, wordseq) {
if (dim(ObsNGrams)[1] != 0) {
# return(1-sum(ObsNGrams[,.(Qbo)]))  # We don't use this formular because End Of Sentence is not counted
return(sum(ObsNGrams[,c-cDis]/Nm1Grams[wordseq, c, on=.(term)]))
} else {
return(1)
}
}
## Find next word
## Return a list of predicted next words according to previous 2 user input words
##
## - xy: character vector containing user-input bigram, separated by a space
## - words_num: number of candidates of next words returned
Find_Next_word <- function(xy, words_num) {
xy <- gsub(" ", "_", xy)
if (length(which(BiFreq$term == xy)) > 0) {  # C(x,y) > 0
## N-grams preparation
# Retrieve all observed trigrams beginning with xy: OT
ObsTriG <- get.obs.NGrams.by.pre(xy, TriFreq)
y <- str_split_fixed(xy,"_", 2)[,2]
# Retrieve all observed bigrams beginning with y: OB
ObsBiG <- get.obs.NGrams.by.pre(y, BiFreq)
# Retrieve all unigrams end the unobserved bigrams UOBT: z where C(y,z) = 0, UOB in UOT
UnObsBiTails <- get.unobs.Ngram.tails(ObsBiG, 2)
# Exclude observed bigrams that also appear in observed trigrams: OB in UOT
ObsBiG <- ObsBiG[!str_split_fixed(ObsTriG[,term], "_", 2)[,2], on="term"]
## Calculation part
# Calculate probabilities of all observed trigrams: P^*(z|x,y)
ObsTriG <- cal.obs.prob(ObsTriG, BiFreq, xy)
# Calculate Alpha(x,y)
Alpha_xy <- cal.alpha(ObsTriG, BiFreq, xy)
# Calculate probabilities of all observed bigrams: P^*(z|y), (y,z) in UOT
ObsBiG <- cal.obs.prob(ObsBiG, UniFreq, y)
# Calculate Alpha(y)
Alpha_y <- cal.alpha(ObsBiG, UniFreq, y)
# Calculate P_{ML}(z), where c(y,z) in UOB: Alpha_y * P_{ML}(z)
UnObsBiTails[, Prob:=UniFreq[UnObsBiTails, c, on=.(term)]/UniFreq[UnObsBiTails, sum(c), on=.(term)]]
UnObsBiTails[, Prob:=Alpha_xy*Alpha_y*Prob]
# Remove unused column in ObsTriG and ObsBiG
ObsTriG[, c("c", "cDis"):=NULL]
ObsTriG[, term:=str_remove(ObsTriG[, term], "([^_]+_)+")]
ObsBiG[, c("c", "cDis"):=NULL]
ObsBiG[, term:=str_remove(ObsBiG[, term], "([^_]+_)+")]
# Compare OT, Alpha_xy * P_{Katz}(z|y)
# P_{Katz}(z|y) = 1. P^*(z|y), 2. Alpha_y * P_{ML}(z)
ObsBiG[,Prob:=Alpha_xy*Prob]
AllTriG <- setorder(rbind(ObsTriG, ObsBiG, UnObsBiTails), -Prob)
return(AllTriG[Prob!=0][1:min(dim(AllTriG[Prob!=0])[1], words_num)])
} else {  # C(x,y) = 0
y <- str_split_fixed(xy,"_", 2)[,2]
# c(y>0)
if (length(which(UniFreq$term == y)) > 0) {
# Retrieve all observed bigrams beginning with y: OB
ObsBiG <- get.obs.NGrams.by.pre(y, BiFreq)
# Calculate probabilities of all observed bigrams: P^*(z|y)
ObsBiG <- cal.obs.prob(ObsBiG, UniFreq, y)
# Calculate Alpha(y)
Alpha_y <- cal.alpha(ObsBiG, UniFreq, y)
# Retrieve all unigrams end the unobserved bigrams UOBT: z where C(y,z) = 0
UnObsBiTails <- get.unobs.Ngram.tails(ObsBiG, 2)
# Calculate P_{ML}(z), where c(y,z) in UOB: Alpha_y * P_{ML}(z)
UnObsBiTails[, Prob:=UniFreq[UnObsBiTails, c, on=.(term)]/UniFreq[UnObsBiTails, sum(c), on=.(term)]]
UnObsBiTails[, Prob:=Alpha_y*Prob]
# Remove unused column in ObsBiG
ObsBiG[, c("c", "cDis"):=NULL]
ObsBiG[, term:=str_remove(ObsBiG[, term], "([^_]+_)+")]
AllBiG <- setorder(rbind(ObsBiG, UnObsBiTails), -Prob)
return(AllBiG[Prob!=0][1:words_num])
} else {  # c(y=0)
# P^*z
return(setorder(UniFreq, -cDis)[1:words_num,.(term, Prob=cDis/UniFreq[,sum(c)])])
}
}
}
## Remove elements not being used by prediction model
Preprocess <- function(wordseq) {
names(wordseq) <- NULL
quest <- wordseq %>%
tokens(remove_numbers=T, remove_punct=T, remove_symbols=T,
remove_hyphens=T, remove_twitter=T, remove_url=T) %>%
tokens() %>%
tokens_remove(stopwords("en")) %>%
tokens_tolower()
return(paste(tail(unlist(quest), 2), collapse = " "))
}
#FINALLY IT is working change quest$text1 to unlist(quest)
#July 16, 2022
Next_word <- function(prephrase, words_num=5) {
bigr <- Preprocess(prephrase)
result <- Find_Next_word(bigr, words_num)
if (dim(result)[1] == 0) {
rbind(result, list("<Please input more text>", 1))
}
return(result)
}
Preprocess("He likes to eat ice")
Next_word("He likes to eat ice")
Preprocess("the prime minister")
Next_word("the prime minister",  words_num=10)
Preprocess("a nuclear power")
Next_word("a nuclear power", words_num=10)
Preprocess("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd")
Next_word("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",  words_num=10)
Preprocess("Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his")
Next_word("Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his", words_num=10)
Preprocess("I'd give anything to see arctic monkeys this")
Next_word("I'd give anything to see arctic monkeys this", words_num=10)
Preprocess("I'm thankful my childhood was filled with imagination and bruises from playing")
Next_word("I'm thankful my childhood was filled with imagination and bruises from playing", words_num=10)
Preprocess("Every inch of you is perfect from the bottom to the")
Next_word("Every inch of you is perfect from the bottom to the", words_num=10)
Preprocess("Talking to your mom has the same effect as a hug and helps reduce your")
Next_word("Talking to your mom has the same effect as a hug and helps reduce your", words_num=10)
Preprocess("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd")
Next_word("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", words_num=10)
shiny::runApp()
getwd()
library(slidify)
library(knitr)
slidify("index.Rmd")
setwd("~/R_exercises/Coursera/Course_10/LanguageModel/LanguageModel_deck")
slidify("index.Rmd")
browserURL("index.Rmd")
browseURL("index.Rmd")
library(knitr)
browseURL("index.Rmd")
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
?rpubsUpload
install.packages("rsconnect")
library(rsconnect)
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
upload_result <- rpubsUpload("A Simple Language Model", "index.html")if (!is.null(result$continueUrl))
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
if (!is.null(result$continueUrl))
browseURL(result$continueUrl)
if (!is.null(upload_result$continueUrl))
browseURL(upload_result$continueUrl)
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
upload_result <- rpubsUpload("A Simple Language Model", "index.html")
if (!is.null(upload_result$continueUrl))
browseURL(upload_result$continueUrl)
